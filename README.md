# Лабораторная работа по курсу "Искусственный интеллект"
# Многослойный персептрон

| Студент | Ермакова Анна Николаевна |
|------|------|
| Группа  | 301 |
| Оценка 1 (свой фреймворк) | *X* |
| Оценка 2 (PyTorch/Tensorflow) | *X* |
| Проверил | Сошников Д.В. |

> *Комментарии проверяющего*
### Задание

Решить задачу классификации для датасетов MNIST, FashionMNIST, CIFAR-10 с помощью 1, 2 и 3-слойного персептрона. Попробовать разные передаточные функции, провести сравнительную оценку решений. Решение сделать двумя способами:
* "С нуля", на основе базовых операций библиотеки numpy. Решение желательно реализовать в виде библиотеки, пригодной для решения более широкго круга задач.
* На основе одного из существующих нейросетевых фреймворков, в соответствии с вариантом задания:
   1. PyTorch
   1. Tensorflow/Keras

> *Номер варианта вычисляется по формуле 1 + (N-1) mod 2, где N - номер студента в списке.*

Решение оформить в файлах [Solution_MyFramework.ipynb](Solution_MyFramework.ipynb) и [Solution.ipynb](Solution.ipynb). 
Отчет по работе и сравнение методов пишется в этом файле после задания.
### Критерии оценки

Первая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация сделана как библиотека для обучения сетей различных конфигураций, в соответствии с примером | 1 |
| Улучшена архитектура библиотеки, отдельно вынесены алгоритмы обучения, функции потерь | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |

Вторая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация использует возможности базового фреймворка, включая работу с данными | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |
| Проведен анализ для другого датасета с цветными картинками (CIFAR-10) | 1 |

## Отчёт по работе

Отчёт, помимо общего описания решения, должен включать:
* График точности на обучающей и проверочной выборке в процессе обучения
* Confusion Matrix
* Сравнение полученных показателей точности модели для различных гиперпараметров (передаточных функций, числа нейронов в слоях и т.д.)


Выделенны классы для каждого слоя class Linear, class Softmax, class CrossEntropyLoss, class Tanh, class Sigmoid, class RELU список которых может дополняться. Описан класс для создания сети class Net. Выделены классы для алгоритмов обучения class  Backpropagation, class  Resilientpropagation список которых может дополняться (реализация приведена только для Backpropagation). Создан класс, удобный для работы с сетью 1 - создание сети 2 - обучение модели на датасете 3 - проверка качества модели. В методах класса учтен ввод всевозможных изменяемых параметров, как для создания сети ,так и для ее обучения. Вынесена возможность выбора алгоритма обучения и функции потерь.

Реализован однослойный персептрон. Обучениие дало точность - accuracy=0.8720238095238095. 

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/1layer.png)

Точность больше 0.85 - хватило одной эпохи, подошли параметры: размер батча - 200, learning rate - 0.1, функция нормировки - Softmax, функция активации - tanh, функция ошибки - CrossEntropy.

Далее были реализованы персептроны с количеством слоев - 1,2,3. 

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/layer1-7.png)

Матрица для сети с количеством слоев - 7: 

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/conf_matr_7_layers.png)

Точность у всех реализаций больше 0.85 - хватило одной эпохи, подошли параметры: размер батча - 200, learning rate - 0.1, функция нормировки - Softmax, функция активации - tanh, функция ошибки - CrossEntropy, слои полносвязные. Оптимальные значение из представленных - 2 или 3 слоя.

Рассмотрены три функции активации: сигмоида, ReLu, tahn. В качестве начальных параметров выбрана двухслойная сеть и параметры : размер батча - 200, количество эпох - 5, learning rate - 0.1, функция нормировки - Softmax, функция ошибки - CrossEntropy, количество нейронов на втором слое - 20.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/sigmoid.png)
![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/relu.png)
![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/tanh.png)

Наглядно видно, что разные функции активации дают разную точность классификации, в данном случае RELU и tanh дали примерно одинаковый результат accuracy (0.92 и 0.92 на тестовой соответственно), в то время как sigmoid оказался хуже (0.88 на тестовой)

Рассмотрено различное число нейронов в промежуточном слое для двухслойного персептрона и параметры: размер батча - 200, количество эпох - 1, learning rate - 0.1, функция нормировки - Softmax, функция ошибки - CrossEntropy, функция активации - Tanh.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/num_neurons.png)

В результате лучшие значеня при количестве нейронов больше 10.

Реализация четырехслойного персептрона с функцией активации - Tanh и количеством нейронов на каждом скрытом уровне - 20.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/res1.png)

Результаты для датасета - FachionMnist:

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/res2.png)

параметры: размер батча - 150, количество эпох - 10, learning rate - 0.1, функция нормировки - Softmax, функция ошибки - CrossEntropy, функция активации - Tanh, три полносвязных слоя.

Используя фреймворк по варианту - PyTorch. Были реализованы классы наследующие от nn.Module, содержащие различные параметры и проведено обучание для датасетов mnist и fashion mnist. Работа с данными была реализована с помощью torchvision.

Для однослойного персептрона точность больше 0.85 - хватило 5 эпох, подошли параметры: размер батча - 200, learning rate - 0.001, функция нормировки - log_Softmax, функция активации - relu, функция ошибки - nll_loss.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/1l.png)

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/1ll.png)

Для четырехслойного персептрона точность больше 0.85 - хватило 5 эпох, подошли параметры: размер батча - 200, learning rate - 0.001, функция нормировки - log_Softmax, функция активации - relu, функция ошибки - nll_loss.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/4l.png)

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/4ll.png)

Сравним различные гиепрпараметры на примере двухслойного персептрона (т.к. даже при одном слое точно получилась высокой, для теста - больше 90%). Рассмотрим такие передаточные функции как: сигмоида, ReLu, tahn, selu.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/reluu.png)

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/tanhh.png)

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/sigmoidd.png)

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/selu.png)

В качестве начальных параметров выбрана двухслойная сеть и параметры : размер батча - 200, количество эпох - 5, количество нейронов на втором слое - 150, learning rate - 0.001, функция нормировки - log_Softmax, функция ошибки - nll_loss. Результаты на тестовой выборке relu - 96.9, tanh - 96.6, sigmoid - 95.1, selu - 96.3.

Рассмотрим различное число слоев на промежуточном уровне.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/nunn.png)

В качестве начальных параметров выбрана двухслойная сеть и параметры : размер батча - 200, количество эпох - 5, learning rate - 0.001, функция нормировки - log_Softmax, функция активации - relu, функция ошибки - nll_loss. Точность становится стобильно 96% начиная с 40 нейронов.

Рассмотрим датасет fashionMNIST.

![alt text](https://github.com/annnn-2/ANN-lab1/blob/main/pics/fashionm.png)

Получили точность выше 85% для четырехслойного персептрона.

Вывод: При создании персептрона используя разные инструменты, я увидела плюсы работы с уже существующими фреймворками, в которых приходится писать меньше кода и не приходится выполнять вручную математические операции. С другой стороны, проблема фреймворков в том, что функций очень много и порой сложно выбрать какая именно подходит для данного случая, в следствии чего приходится много работать с документацией. Интересно было визуализировать процесс обучения сети , при некоторых попытках возникали аномалии - точность переставала меняться или ухудшалась, значит происходило переобучение. Другая проблема с которой я столкнулась это размер тензоров - необходимо понимать и внимательно следить за размером, который требуется в той или иной функции.Было инетересно проделать эти две работы и на практике сравнить разного уровня средства реализации и обучения нейронных сетей.

## Codespaces

По возможности, используйте GitHub Codespaces для выполнения работы. По результатам, дайте обратную связь:
1. Что понравилось?
1. Что не понравилось?
1. Какие ошибки или существенные затруднения в работе вы встречали? (По возможности, будьте как можно более подробны, указывайте шаги для воспроизведения ошибок)

## Материалы для изучения

 * [Реализация своего нейросетевого фреймворка](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroMyFw.ipynb)
 * [Введение в PyTorch](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroPyTorch.ipynb)
 * [Введение в Tensorflow/Keras](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroKerasTF.ipynb)
